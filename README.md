colab link :- https://colab.research.google.com/drive/1byc0gJDLRd7o_35KXdBIlU8KxnhNpeDW?usp=sharing
Raw dataset link :- https://drive.google.com/drive/folders/1YSX-gHFVuwFQiyF1HL5odyEq7ggooQyx?usp=sharing

download above dataset to run the colab.(Also dataset in the repo will be required so upload both before running colab)

# ðŸ§¬ Bioinformatics â€“ ML Challenge

---

## ðŸ” Data Understanding


### ðŸ§‘â€âš•ï¸ 1. Clinical Data
- **Patient Attributes:**  
  Include survival status, overall survival months, age at diagnosis, and therapy details.  
- **Sample Attributes:**  
  Provide mapping between patient IDs and biological samples (tumor/normal).  
- **Purpose:**  
  Enables correlation between **genomic profiles** and **clinical outcomes**, forming the foundation for survival and subtype analysis.



### ðŸ§« 2. Genomic Alteration Data
- **Copy Number Alterations (CNA):**  
  Used to identify **gene amplifications or deletions** potentially driving tumorigenesis.  
- **Mutation Data (MAF):**  
  Contains **point mutations** and **small insertions/deletions** across key cancer-associated genes.  
- **Gene Panel Matrix:**  
  Acts as a **binary mask** indicating which genes were sequenced or profiled per sample.



### ðŸ§¬ 3. Transcriptomic & Epigenetic Data
- **mRNA Expression (Raw + Z-Score):**  
  Quantifies **gene expression variation**, valuable for **subtype classification** and **clustering analysis**.  
- **Methylation (Promoters):**  
  Captures **epigenetic regulation** by measuring promoter methylation levels â€” reflecting **gene silencing** or **activation patterns** in tumor versus normal tissues.

---

### ðŸ”¹ Relation between Age of patient and Type of Cancer

![Age vs. Cancer Type](Age_vs_CancerType.png)

---

### ðŸ”¹ Cancer Type Distribution

![Cancer Type Distribution](Cancer_Distribution.png)

---

### ðŸ”¹ Tumor Size vs Grade

![Grade vs Tumor Size](Grade_vs_Tumor_Size.png)

---

# ðŸ§¬ Data Preprocessing Pipeline

To build a robust breast cancer subtype prediction model, the dataset was split into **clinical** and **gene expression** features and processed to ensure quality and reduce dimensionality.

---

## ðŸ”¹ Clinical Features

### SimpleImputer(strategy=median) 
Examples: AGE, NPI, LYMPH_NODES_EXAMINED_POSITIVE  
- Imputation: Missing values filled using median.  
- Scaling: StandardScaler applied for normalized input.

### OneHotEncoding
Examples: CELLULARITY, CHEMOTHERAPY, HORMONE_THERAPY  
- Encoding: One-Hot for nominal, Ordinal for ordered categories.


## ðŸ”¹ Gene Expression Features

~20K gene columns reduced using hybrid techniques:  
1. Variance Threshold â†’ remove low-variance genes  
2. ANOVA F-Test â†’ select statistically significant genes  
3. PCA â†’ capture main variance patterns  
4. RFE (Random Forest) â†’ retain top-ranked features



## ðŸŽ¯ Outcome

- Integrated clinical + gene features  
- Reduced features from 20,639 â†’ 8363 informative  
- Noise-free dataset ready for high-accuracy classification

---

## ðŸ¤– Model Building and Optimization

### ðŸŒ² Random Forest Classifier  
We chose the **Random Forest** algorithm as our baseline model for classification due to its robustness, interpretability, and high performance on tabular biomedical datasets.  
It is an **ensemble learning** method that constructs multiple decision trees during training and combines their predictions to improve generalization.

#### ðŸ”¹ Why Random Forest?
- Handles both **numerical and categorical** data effectively  
- Automatically performs **feature importance ranking**  
- Reduces risk of **overfitting** compared to individual decision trees  
- Works well even with **imbalanced** and **high-dimensional** genomic datasets  
- Provides strong **baseline accuracy** without extensive tuning  

### âš™ï¸ Hyperparameter Tuning with Grid Search CV  
To further enhance model performance, we applied **Random Search Cross-Validation (RandomSearchCV)** with **5-fold CV** to systematically explore hyperparameter combinations such as:  
**Best Parameters after tuning:**  
```python
{'n_estimators': 200, 
 'min_samples_split': 5, 
 'min_samples_leaf': 1, 
 'max_features': 'sqrt', 
 'max_depth': 10}  
```
This process ensures an optimal balance between **bias and variance**, improving model generalization.

### ðŸ“ˆ Model Performance  
After tuning, the optimized Random Forest achieved:  
- **Accuracy:(Before RandomSearchCV)** 90.836%
- **Accuracy:(After RandomSearchCV)** 91.36% 
- **F1_Score:(Before RandomSearchCV)** 0.89433
- **F1_Score:(After RandomSearchCV)** 0.9203
- **Precision and Recall:** Both remained consistently high, indicating balanced predictions  
- **Feature Insights:** Top contributing features included key clinical indicators and selected gene expression markers

---

## âš ï¸ Challenges Faced

During the project, several challenges were encountered that required careful consideration:

### 1ï¸âƒ£ Data Segregation and Collection
- The dataset included **clinical, genomic, and transcriptomic features** with different formats and identifiers.  
- Mapping patients to samples and integrating multiple data types required extensive preprocessing and cleaning.  

### 2ï¸âƒ£ High Dimensionality and Feature Selection
- Gene expression data contained **~20,000 features**, which can lead to **noise and computational overhead**.  
- We applied hybrid dimensionality reduction techniques (Variance Threshold, ANOVA F-Test, PCA, RFE) to retain biologically relevant features.  

### 3ï¸âƒ£ Model Selection and Overfitting
- Choosing an appropriate model that balances **accuracy and generalization** was critical.  
- Random Forest helped mitigate overfitting, but **hyperparameter tuning** with Random Search CV was required to achieve optimal performance.  

### 4ï¸âƒ£ Imbalanced and Missing Data
- Some categorical and numeric clinical variables had **missing values**, while cancer subtypes were **unevenly represented**.  
- Strategies like median imputation, encoding, and careful cross-validation were employed to handle these issues.  

### 5ï¸âƒ£ Computational Constraints
- Handling high-dimensional omics data required **efficient preprocessing** and **feature reduction** to make model training feasible on standard hardware.  

> ðŸ’¡ *Overall, addressing these challenges ensured a robust, interpretable, and high-performing breast cancer subtype prediction model.*

## Possible Improvements
- To improve model performance this model can be combined with XGBOOST to make a Hybrid model





